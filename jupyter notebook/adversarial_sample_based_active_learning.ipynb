{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3881c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up random seed\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e83c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handler\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class Messidor_Handler(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.RandomRotation(15),\n",
    "                                             transforms.RandomResizedCrop(size=512, scale=(0.9, 1)),\n",
    "                                             transforms.RandomHorizontalFlip(),\n",
    "                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.X[index], self.Y[index]\n",
    "        x = Image.fromarray(np.uint8(x))\n",
    "        x = self.transform(x)\n",
    "        return x, y, index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "class Breast_Handler(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.RandomRotation(15),\n",
    "                                             transforms.RandomResizedCrop(size=512, scale=(0.9, 1)),\n",
    "                                             transforms.RandomHorizontalFlip(),\n",
    "                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.X[index], self.Y[index]\n",
    "        x = Image.fromarray(np.uint8(x))\n",
    "        x = self.transform(x)\n",
    "        return x, y, index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "class Breast_multi_Handler(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.RandomRotation(15),\n",
    "                                             transforms.RandomResizedCrop(size=512, scale=(0.9, 1)),\n",
    "                                             transforms.RandomHorizontalFlip(),\n",
    "                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.X[index], self.Y[index]\n",
    "        x = Image.fromarray(np.uint8(x))\n",
    "        x = self.transform(x)\n",
    "        return x, y, index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1ee33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "# setup_seed(2022)\n",
    "input_path = \"./data/Messidor_Dataset/Annotation\"\n",
    "normal=[]\n",
    "diseased=[]\n",
    "\n",
    "for i in glob.glob(os.path.join(input_path,\"*.xls\")):\n",
    "    df_data = pd.read_excel(i)\n",
    "    normal_path = df_data.loc[(df_data[\"Retinopathy grade\"] == 0)][\"Image name\"].values\n",
    "    for j in normal_path:\n",
    "        normal.append(\"./data/Messidor_Dataset/\"+i[-10:-4]+\"/\"+j)\n",
    "    diseased_path = df_data.loc[(df_data[\"Retinopathy grade\"] != 0)][\"Image name\"].values\n",
    "    for k in diseased_path:\n",
    "        diseased.append(\"./data/Messidor_Dataset/\"+i[-10:-4]+\"/\"+k)\n",
    "print(len(normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878a8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import PIL\n",
    "import cv2\n",
    "# setup_seed(2022)\n",
    "\n",
    "image_size=512\n",
    "labels = []\n",
    "dataset = []\n",
    "\n",
    "def crop(image): \n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    return image\n",
    "    \n",
    "def create_dataset(image_category,label):\n",
    "\n",
    "    for image_path in tqdm(image_category):\n",
    "        # image = PIL.Image.open(image_path)\n",
    "        image = cv2.imread(image_path,cv2.IMREAD_COLOR)\n",
    "        image = crop(image)\n",
    "        image = np.array(image)\n",
    "        image = cv2.resize(image,(image_size,image_size))#299,299,3\n",
    "        # image = transform(image)\n",
    "        dataset.append([np.array(image),np.array(label)])\n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(x,y):\n",
    "    x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2,random_state=2022)\n",
    "    return x_train,torch.LongTensor(y_train),x_val,torch.LongTensor(y_val)\n",
    "\n",
    "\n",
    "def split_train_val(x_train,y_train):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size=0.2,random_state=2022)\n",
    "    return x_train,torch.LongTensor(y_train),x_test,torch.LongTensor(y_test)\n",
    "# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# x_val,x_test,y_val,y_test = train_test_split(x_val,y_val,test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0859ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data augumentation\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor,ToPILImage\n",
    "# import albumentations as A\n",
    "import torch\n",
    "\n",
    "def transform(x,y):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomResizedCrop(size=512, scale=(0.9, 1)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    x_train=np.array(x).astype(np.float32)\n",
    "    y_train=np.array(y)\n",
    "    for i in range(0,len(x_train)):\n",
    "        x = transform(x_train[i])\n",
    "        X.append(x)\n",
    "        Y.append(y_train[i])\n",
    "    X=np.array(x_train)\n",
    "    Y=np.array(y_train)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4738884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "#读取数据过程+结合handler\n",
    "# setup_seed(2022)\n",
    "class Data:\n",
    "    def __init__(self, X_train, Y_train, X_test, Y_test, handler):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        # self.X_val = X_val\n",
    "        # self.Y_val = Y_val\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.handler = handler\n",
    "        \n",
    "        self.n_pool = len(X_train)\n",
    "        self.n_test = len(X_test)\n",
    "        \n",
    "        self.labeled_idxs = np.zeros(self.n_pool, dtype=bool)\n",
    "\n",
    "    def supervised_training_labels(self):\n",
    "        # generate initial labeled pool 根据label index来确认 labeled_idxs就是label data反之是unlabel data（x_train+y_train)\n",
    "        tmp_idxs = np.arange(self.n_pool)\n",
    "        self.labeled_idxs[tmp_idxs[:]] = True\n",
    "        \n",
    "    def initialize_labels_random(self, num):\n",
    "        # generate initial labeled pool 根据label index来确认 labeled_idxs就是label data反之是unlabel data（x_train+y_train)\n",
    "        tmp_idxs = np.arange(self.n_pool)\n",
    "        np.random.shuffle(tmp_idxs)\n",
    "        self.labeled_idxs[tmp_idxs[:num]] = True\n",
    "        \n",
    "    def initialize_labels(self, dataset, num):\n",
    "        tmp_idxs = np.arange(self.n_pool)\n",
    "        np.random.shuffle(tmp_idxs)\n",
    "        net_init = get_net(args.dataset_name, device, init=True) \n",
    "        strategy_init = get_strategy(\"KMeansSampling\")(dataset, net_init, init=True) \n",
    "        rd = 0\n",
    "        strategy_init.train(rd, init=True)\n",
    "        query_idxs = strategy_init.query(args.n_init_labeled)\n",
    "        strategy.update(query_idxs)\n",
    "    \n",
    "    def get_labeled_data(self):\n",
    "        labeled_idxs = np.arange(self.n_pool)[self.labeled_idxs]\n",
    "        print(\"labeled data\",labeled_idxs.shape)\n",
    "        return labeled_idxs, self.handler(self.X_train[labeled_idxs], self.Y_train[labeled_idxs])\n",
    "    \n",
    "    def get_unlabeled_data(self):\n",
    "        unlabeled_idxs = np.arange(self.n_pool)[~self.labeled_idxs]\n",
    "        return unlabeled_idxs, self.handler(self.X_train[unlabeled_idxs], self.Y_train[unlabeled_idxs])\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.labeled_idxs.copy(), self.handler(self.X_train, self.Y_train)\n",
    "    \n",
    "    def get_val_data(self):\n",
    "        return self.handler(self.X_val, self.Y_val)\n",
    "        \n",
    "    def get_test_data(self):\n",
    "        return self.handler(self.X_test, self.Y_test)\n",
    "    \n",
    "    def cal_test_acc(self, preds):\n",
    "        return 1.0 * (self.Y_test==preds).sum().item() / self.n_test\n",
    "    \n",
    "    def cal_train_acc(self, preds):\n",
    "        return 1.0 * (self.Y_train==preds).sum().item() / self.n_pool\n",
    "    \n",
    "    def add_labeled_data(self,data,label):\n",
    "        data = torch.reshape(data, (512,512,3))\n",
    "        data = torch.unsqueeze(data, 0)\n",
    "        self.X_train = torch.tensor(self.X_train)\n",
    "        self.Y_train = torch.tensor(self.Y_train)\n",
    "        self.X_train = torch.cat((self.X_train, data), 0)\n",
    "        self.Y_train = torch.cat((self.Y_train, torch.tensor([label])), 0)\n",
    "        \n",
    "        self.labeled_idxs=np.append(self.labeled_idxs,True)\n",
    "        self.n_pool+=1\n",
    "        \n",
    "    def update_pseudo_label(self,idx,label):\n",
    "        self.X_train = torch.tensor(self.X_train)\n",
    "        self.Y_train[idx] = label\n",
    "    \n",
    "#     def (self,data,label):\n",
    "#         data = tadd_labeled_dataorch.reshape(data, (-1,512,512,3))\n",
    "# #         data = torch.unsqueeze(data, 0)\n",
    "#         self.X_train = torch.tensor(self.X_train)\n",
    "#         self.Y_train = torch.tensor(self.Y_train)\n",
    "#         self.X_train = torch.cat((self.X_train, data), 0)\n",
    "#         self.Y_train = torch.cat((self.Y_train, label), 0)\n",
    "#         for i in range(len(data)):\n",
    "#             self.labeled_idxs=np.append(self.labeled_idxs,True)\n",
    "#             self.n_pool+=1\n",
    "    \n",
    "    def get_label(self, idx):\n",
    "        self.Y_train=np.array(self.Y_train)\n",
    "        label = torch.tensor(self.Y_train[idx])\n",
    "#         print(label)\n",
    "        return label#所有label\n",
    "\n",
    "    def get_efficient_training_data(self,idx, new_idx):\n",
    "        all_idx = np.concatenate((idx, new_idx), axis=None)\n",
    "        labeled_idxs = np.arange(self.n_pool)[all_idx]\n",
    "        return labeled_idxs, self.handler(self.X_train[labeled_idxs], self.Y_train[labeled_idxs])\n",
    "\n",
    "def get_Messidor(handler):\n",
    "    # dataset = create_dataset(normal,1)\n",
    "    # dataset = create_dataset(diseased,0)\n",
    "    # print(len(dataset))\n",
    "    # x = np.array([i[0] for i in dataset])\n",
    "    # y = np.array([i[1] for i in dataset])\n",
    "    # x = torch.Tensor(x)\n",
    "    # y = torch.Tensor(y)\n",
    "    # torch.save(torch.Tensor(x),'./data/Messidor_Crop_x.pt')\n",
    "    # torch.save(torch.Tensor(y),'./data/Messidor_Crop_y.pt')\n",
    "    # x=torch.load('./data/Messidor_Dataset/x.pt')\n",
    "    # y=torch.load('./data/Messidor_Dataset/y.pt')\n",
    "    x=torch.load('./data/Messidor_Crop_x.pt')\n",
    "    y=torch.load('./data/Messidor_Crop_y.pt')\n",
    "    y=y.long()\n",
    "    x,y = transform(x,y)\n",
    "#     x=torch.load('./data/Messidor_transform_x.pt')\n",
    "#     y=torch.load('./data/Messidor_transform_y.pt')\n",
    "#     y=y.long()\n",
    "    x_train,y_train,x_test,y_test = split_dataset(x,y)\n",
    "    x_train,y_train,x_val,y_val = split_train_val(x_train,y_train)\n",
    "    print(\"x_train\",np.array(x_train).shape)\n",
    "    print(\"x_val\",np.array(x_val).shape)\n",
    "    print(\"x_test\",np.array(x_test).shape)\n",
    "    return x_train,y_train,x_val, y_val, x_test,y_test,handler\n",
    "\n",
    "\n",
    "def walk_root_dir(DIR):\n",
    "    wholePathes=[]\n",
    "    for dirpath, subdirs, files in os.walk(DIR):\n",
    "        for x in files:\n",
    "            if x.endswith(('.tif')):\n",
    "                wholePathes.append(os.path.join(dirpath, x))\n",
    "    return wholePathes\n",
    "\n",
    "def Dataset_loader(input_path, RESIZE):\n",
    "    IMG = []\n",
    "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
    "    wholePathes=walk_root_dir(input_path)\n",
    "    for PATH in tqdm(wholePathes):\n",
    "        img = read(PATH)\n",
    "        img = cv2.resize(img, (RESIZE,RESIZE))\n",
    "        IMG.append(np.array(img))\n",
    "    return IMG\n",
    "\n",
    "def get_Breast(handler):\n",
    "#     benign = np.array(Dataset_loader('./breast/histology_slides/breast/benign/SOB',512))\n",
    "#     malign = np.array(Dataset_loader('./breast/histology_slides/breast/malignant/SOB',512))\n",
    "#     # Create labels\n",
    "#     benign_label = np.zeros(len(benign))\n",
    "#     malign_label = np.ones(len(malign))\n",
    "\n",
    "#     # Merge data \n",
    "#     x = np.concatenate((benign, malign), axis = 0)\n",
    "#     y = np.concatenate((benign_label, malign_label), axis = 0)\n",
    "    \n",
    "#     torch.save(torch.Tensor(x),'./breast/histology_slides/breast/x.pt')\n",
    "#     torch.save(torch.Tensor(y),'./breast/histology_slides/breast/y.pt')\n",
    "    \n",
    "    # x=torch.load('./breast/x.pt')\n",
    "    # y=torch.load('./breast/y.pt')\n",
    "    y=y.long()\n",
    "\n",
    "    x_train,y_train,x_test,y_test = split_dataset(x,y)\n",
    "    x_train,y_train,x_val,y_val = split_train_val(x_train,y_train)\n",
    "    print(\"x_train\",np.array(x_train).shape)\n",
    "    print(\"x_val\",np.array(x_val).shape)\n",
    "    print(\"x_test\",np.array(x_test).shape)\n",
    "    return x_train,y_train,x_val, y_val, x_test,y_test,handler\n",
    "\n",
    "\n",
    "def get_Breast_multi(handler):\n",
    "    benign = np.array(Dataset_loader('./Breast_Cancer_Diagnosis_Dataset/ICIAR2018_BACH_Challenge/Photos/Benign',512))\n",
    "    insitu = np.array(Dataset_loader('./Breast_Cancer_Diagnosis_Dataset/ICIAR2018_BACH_Challenge/Photos/InSitu',512))\n",
    "    invasive = np.array(Dataset_loader('./Breast_Cancer_Diagnosis_Dataset/ICIAR2018_BACH_Challenge/Photos/Invasive',512))\n",
    "    normal = np.array(Dataset_loader('./Breast_Cancer_Diagnosis_Dataset/ICIAR2018_BACH_Challenge/Photos/Normal',512))\n",
    "    # Create labels\n",
    "    benign_label = np.zeros(len(benign))\n",
    "    insitu_label = np.ones(len(insitu))\n",
    "    invasive_label = np.full(len(invasive),2)\n",
    "    normal_label = np.full(len(normal),3)\n",
    "\n",
    "    # Merge data \n",
    "    x = np.concatenate((benign, insitu, invasive, normal), axis = 0)\n",
    "    y = np.concatenate((benign_label, insitu_label, invasive_label, normal_label), axis = 0)\n",
    "    \n",
    "    # torch.save(torch.Tensor(x),'./Breast_Cancer_Diagnosis_Dataset/x.pt')\n",
    "    # torch.save(torch.Tensor(y),'./Breast_Cancer_Diagnosis_Dataset/y.pt')\n",
    "    \n",
    "    # x=torch.load('./Breast_Cancer_Diagnosis_Dataset/x.pt')\n",
    "    # y=torch.load('./Breast_Cancer_Diagnosis_Dataset/y.pt')\n",
    "    x=torch.Tensor(x)\n",
    "    y=torch.Tensor(y)\n",
    "    y=y.long()\n",
    "\n",
    "    x_train,y_train,x_test,y_test = split_dataset(x,y)\n",
    "    # x_train,y_train,x_val,y_val = split_train_val(x_train,y_train)\n",
    "    print(\"x_train\",np.array(x_train).shape)\n",
    "    # print(\"x_val\",np.array(x_val).shape)\n",
    "    print(\"x_test\",np.array(x_test).shape)\n",
    "    return x_train,y_train,x_test,y_test,handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eef694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "# setup_seed(2022)\n",
    "class Net:\n",
    "    def __init__(self, net, params, device):\n",
    "        self.net = net\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "  \n",
    "    def train(self, data):\n",
    "        n_epoch = 1\n",
    "        self.clf = self.net().to(self.device)\n",
    "        self.clf.train()\n",
    "#         if len(data)<=300:\n",
    "#             n_epoch = 20\n",
    "#         else:\n",
    "#             n_epoch = 30\n",
    "            \n",
    "        # optimizer = optim.SGD(self.clf.parameters(), momentum=0.9, lr=0.0001,weight_decay=0.0005, nesterov=True)\n",
    "        optimizer = optim.Adam(self.clf.parameters(), lr=0.0001,weight_decay=0.0005)\n",
    "        # lr = 0.0001\n",
    "        # lr = 0.000001 * len(data)#变化的lr\n",
    "        # if(lr > 0.0003):\n",
    "        #     lr = self.params['optimizer_args']['lr']\n",
    "        # optimizer = optim.Adam(self.clf.parameters(),lr)#,weight_decay=1e-5\n",
    "        \n",
    "        loader = DataLoader(data, shuffle=True, **self.params['train_args'])\n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            for batch_idx, (x, y, idxs) in enumerate(loader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out, e1 = self.clf(x)\n",
    "                loss = F.cross_entropy(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "    def init_train(self, data):\n",
    "        n_epoch = self.params['n_epoch']\n",
    "        best = {'epoch':1,'loss':10}   \n",
    "        self.clf = self.net().to(self.device)\n",
    "        self.clf.train()\n",
    "        optimizer = optim.Adam(self.clf.parameters(), lr=0.005)\n",
    "        criterion = nn.MSELoss()\n",
    "        loader = DataLoader(data, batch_size=16, shuffle=True)\n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            for batch_idx, (x, y, idxs) in enumerate(loader):\n",
    "                self.clf.train()\n",
    "                x = x.view(x.size(0), -1)\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out, e1 = self.clf(x)\n",
    "                loss = criterion(out, x)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss+=loss#一个epoch的loss\n",
    "            trigger+=1\n",
    "            if train_loss/(batch_idx+1)<best['loss']:\n",
    "                trigger=0\n",
    "                best['epoch'] = epoch\n",
    "                best['loss'] = train_loss/(batch_idx+1)\n",
    "                torch.save(self.clf, './autoencoder.pth') \n",
    "            train_loss=0\n",
    "            if trigger>=args.early_stop:\n",
    "                break\n",
    "                \n",
    "#以accuracy饱和为早停标准\n",
    "    def supervised_train_acc(self, data):\n",
    "        best = {'epoch':1,'acc':0.5}  \n",
    "        n_epoch = 300\n",
    "        train_acc = 0\n",
    "        trigger=0\n",
    "        stop=100\n",
    "        once=False\n",
    "        self.clf = self.net().to(self.device)\n",
    "        self.clf.train()\n",
    "        # optimizer = optim.Adam(self.clf.parameters(),**self.params['optimizer_args'])#,weight_decay=1e-5\n",
    "        optimizer = optim.Adam(self.clf.parameters(), lr=0.0002, betas=(0.9, 0.999), eps=0.1, weight_decay=0.01)\n",
    "#         optimizer = optim.SGD(self.clf.parameters(), momentum=0.9, lr=0.0003, weight_decay=0.01, nesterov=True)\n",
    "    \n",
    "        # optimizer = optim.SGD(self.clf.parameters(), **self.params['optimizer_args'])\n",
    "        loader = DataLoader(data, shuffle=True, **self.params['train_args'])\n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            for batch_idx, (x, y, idxs) in enumerate(loader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out, e1 = self.clf(x)\n",
    "                pred = out.max(1)[1]\n",
    "                loss = F.cross_entropy(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc += 1.0 * (y==pred).sum().item() / len(x)\n",
    "            train_acc = train_acc/(batch_idx+1)\n",
    "#             trigger+=1\n",
    "            if train_acc==1:\n",
    "                break\n",
    "            train_acc = 0\n",
    "                   \n",
    "    def supervised_val_loss(self, rd, data, val_data):\n",
    "        n_epoch = 150\n",
    "        trigger=0\n",
    "        best = {'epoch':1,'loss':10}        \n",
    "        validation_loss = 0\n",
    "        self.clf = self.net().to(self.device)\n",
    "        self.clf.train()\n",
    "        # if rd==0:\n",
    "        #     self.clf = self.clf\n",
    "        # else:\n",
    "        #     self.clf = torch.load('./model.pth') \n",
    "        # optimizer = optim.SGD(self.clf.parameters(), **self.params['optimizer_args'])\n",
    "#         optimizer = optim.Adam(self.clf.parameters(), **self.params['optimizer_args'])\n",
    "#         optimizer = optim.SGD(self.clf.parameters(), momentum=0.9, lr=0.0003, weight_decay=0.01, nesterov=True)\n",
    "        optimizer = optim.Adam(self.clf.parameters(), lr=0.0002, eps=0.1,weight_decay=0.01)\n",
    "        loader = DataLoader(data, shuffle=True, **self.params['train_args'])\n",
    "        val_loader = DataLoader(val_data, shuffle=False, **self.params['val_args'])\n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            for batch_idx, (x, y, idxs) in enumerate(loader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out, e1 = self.clf(x)\n",
    "                loss = F.cross_entropy(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                self.clf.eval()\n",
    "                for valbatch_idx,(valinputs,valtargets, idxs) in enumerate(val_loader):\n",
    "                    valinputs,valtargets=valinputs.to(device),valtargets.to(device)\n",
    "                    valoutputs, e1=self.clf(valinputs)\n",
    "                    validation_loss+= F.cross_entropy(valoutputs,valtargets.long())\n",
    "            trigger+=1\n",
    "            #early stopping condition: if the acc not getting larger for over 10 epochs, stop\n",
    "            if validation_loss/(valbatch_idx+1)<best['loss']:\n",
    "                trigger=0\n",
    "                best['epoch'] = epoch\n",
    "                best['loss'] = validation_loss/(valbatch_idx+1)\n",
    "                torch.save(self.clf, './model.pth')  \n",
    "            validation_loss = 0\n",
    "            if trigger>=10:\n",
    "                break\n",
    "\n",
    "    def supervised_train(self, rd, data, val_data):\n",
    "        n_epoch = 100\n",
    "        trigger=0\n",
    "        best = {'epoch':1,'acc':0.5}        \n",
    "        val_acc = 0\n",
    "        self.clf = self.net().to(self.device)\n",
    "        # if rd==0:\n",
    "        #     self.clf = self.clf\n",
    "        # else:\n",
    "        #     self.clf = torch.load('./model.pth') \n",
    "        loader = DataLoader(data, shuffle=True, **self.params['train_args'])\n",
    "        val_loader = DataLoader(val_data, shuffle=False, **self.params['val_args'])\n",
    "#         lr = 0.000001 * len(data)#变化的lr\n",
    "#         if(lr > self.params['optimizer_args']['lr']):\n",
    "#             lr = self.params['optimizer_args']['lr']\n",
    "#         optimizer = optim.Adam(self.clf.parameters(),lr)#,weight_decay=1e-5\n",
    "        \n",
    "        # optimizer = optim.Adam(self.clf.parameters(), **self.params['optimizer_args'])\n",
    "        # optimizer = optim.SGD(self.clf.parameters(), **self.params['optimizer_args'])\n",
    "        optimizer = optim.SGD(self.clf.parameters(), momentum=0.9, lr=0.0003, weight_decay=0.01, nesterov=True)\n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            for batch_idx, (x, y, idxs) in enumerate(loader):\n",
    "                self.clf.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out, e1 = self.clf(x)\n",
    "                loss = F.cross_entropy(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.clf.eval()\n",
    "                for valbatch_idx,(valinputs,valtargets, idxs) in enumerate(val_loader):\n",
    "                    valinputs,valtargets=valinputs.to(device),valtargets.to(device)\n",
    "                    valoutputs, e1=self.clf(valinputs)\n",
    "#                         validation_loss+=criterion(valoutputs,valtargets.long())\n",
    "                    pred = valoutputs.max(1)[1]\n",
    "                    val_acc += 1.0 * (valtargets==pred).sum().item() / len(valinputs)\n",
    "                # print(\"epoch: \",epoch,\"val_acc: \",val_acc/(valbatch_idx+1))\n",
    "            trigger+=1\n",
    "            #early stopping condition: if the acc not getting larger for over 10 epochs, stop\n",
    "            if val_acc/(valbatch_idx+1)>best['acc']:\n",
    "                trigger=0\n",
    "                best['epoch'] = epoch\n",
    "                best['acc'] = val_acc/(valbatch_idx+1)\n",
    "                torch.save(self.clf, './model.pth')  \n",
    "            val_acc = 0\n",
    "            if trigger>=10:\n",
    "                break\n",
    "        print(\"best performance at Epoch :{}, acc :{}\".format(best['epoch'],best['acc']))    \n",
    "        \n",
    "    def predict(self, data):\n",
    "#         self.clf = torch.load('./model.pth') \n",
    "        self.clf.eval()\n",
    "        preds = torch.zeros(len(data), dtype=data.Y.dtype)\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out, e1 = self.clf(x)\n",
    "                pred = out.max(1)[1]\n",
    "                preds[idxs] = pred.cpu()\n",
    "        return preds\n",
    "    \n",
    "    def get_underfit_idx(self, data):#如果train_acc=1就不会有underfit_idx\n",
    "        unfit_idxs=[]\n",
    "        self.clf.eval()\n",
    "        preds = torch.zeros(len(data), dtype=data.Y.dtype)\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out, e1 = self.clf(x)\n",
    "                pred = out.max(1)[1]\n",
    "                preds[idxs] = pred.cpu()\n",
    "                if preds[idxs]!=y:\n",
    "                    unfit_idxs.append(idxs)\n",
    "        return unfit_idxs\n",
    "    \n",
    "    def predict_prob(self, data):#将softmax的输出当做probs probs越大说明certrainty越低\n",
    "#         self.clf = torch.load('./model.pth') \n",
    "        self.clf.eval()\n",
    "        probs = torch.zeros([len(data), 4])\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out, e1 = self.clf(x)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                probs[idxs] = prob.cpu()\n",
    "        return probs\n",
    "    \n",
    "    def predict_prob_dropout(self, data, n_drop=10):#相当于求10次的平均\n",
    "        self.clf.train()\n",
    "        probs = torch.zeros([len(data), len(np.unique(data.Y))])\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        for i in range(n_drop):\n",
    "            with torch.no_grad():\n",
    "                for x, y, idxs in loader:\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "                    out, e1 = self.clf(x)\n",
    "                    prob = F.softmax(out, dim=1)\n",
    "                    probs[idxs] += prob.cpu()\n",
    "        probs /= n_drop\n",
    "        return probs\n",
    "    \n",
    "    def predict_prob_dropout_split(self, data, n_drop=10):#求10次和\n",
    "        self.clf.train()\n",
    "        probs = torch.zeros([n_drop, len(data), len(np.unique(data.Y))])\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        for i in range(n_drop):\n",
    "            with torch.no_grad():\n",
    "                for x, y, idxs in loader:\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "                    out, e1 = self.clf(x)\n",
    "                    prob = F.softmax(out, dim=1)\n",
    "                    probs[i][idxs] += F.softmax(out, dim=1).cpu()\n",
    "        return probs\n",
    "    \n",
    "    def get_embeddings(self, data):#不经过最后一层fc的概率\n",
    "        self.clf.eval()\n",
    "        embeddings = torch.zeros([len(data), self.clf.get_embedding_dim()])\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out, e1 = self.clf(x)\n",
    "                embeddings[idxs] = e1.cpu()\n",
    "        return embeddings\n",
    "    \n",
    "    def get_autofeature(self, data):\n",
    "        self.clf = torch.load('./autoencoder.pth') \n",
    "        self.clf.eval()\n",
    "        embeddings = torch.zeros([len(data), self.clf.get_embedding_dim()])\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['test_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out, e1 = self.clf(x)\n",
    "                embeddings[idxs] = e1.cpu()\n",
    "        return embeddings\n",
    "\n",
    "#监督学习baseline\n",
    "class Dense_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dense_Net, self).__init__()\n",
    "        self.feature_extractor=torchvision.models.densenet201(pretrained=True)\n",
    "        self.feature_extractor.classifier=nn.Sequential()\n",
    "        self.fc1 = nn.Linear(1920, 128, bias=True)\n",
    "        self.fc2 = nn.Linear(128, 2, bias=True)\n",
    "        # self.fc3 = nn.Linear(50, 2, bias=True)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(-1, 1920)\n",
    "        e1 = F.relu(self.fc1(x))\n",
    "        x = F.dropout(e1, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x, e1\n",
    "    \n",
    "    def get_embedding_dim(self):\n",
    "        return 128\n",
    "    \n",
    "# class Res_Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Res_Net, self).__init__()\n",
    "#         self.feature_extractor=torchvision.models.resnet18(pretrained=True)\n",
    "#         self.feature_extractor.fc=nn.Sequential()\n",
    "#         self.fc1 = nn.Linear(512, 128, bias=True)\n",
    "#         self.fc2 = nn.Linear(128, 2, bias=True)\n",
    "#         self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.feature_extractor(x)\n",
    "#         x = x.view(-1, 512)\n",
    "#         e1 = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(e1, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return x, e1\n",
    "#     def get_embedding_dim(self):\n",
    "#         return 128\n",
    "class Res_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Res_Net, self).__init__()\n",
    "        self.feature_extractor=torchvision.models.resnet18(pretrained=True)\n",
    "        self.feature_extractor.fc=nn.Sequential()\n",
    "        self.fc1 = nn.Linear(512, 2, bias=True)\n",
    "        # self.fc2 = nn.Linear(128, 2, bias=True)\n",
    "        # self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e1 = self.feature_extractor(x)\n",
    "        # x = x.view(-1, 512)\n",
    "        x = self.fc1(e1)\n",
    "        # x = F.dropout(e1, training=self.training)\n",
    "        # x = self.fc2(x)\n",
    "        return x,e1\n",
    "    def get_embedding_dim(self):\n",
    "        return 128\n",
    "\n",
    "    \n",
    "class Inception_V3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Inception_V3, self).__init__()\n",
    "        # self.transform_input = True\n",
    "        # get layers of baseline model, loaded with some pre-trained weights\n",
    "        model = torchvision.models.inception_v3(pretrained=True,transform_input=True, aux_logits=False)\n",
    "\n",
    "        # define our model\n",
    "        self.inception_layers = nn.Sequential(\n",
    "            OrderedDict(list(model.named_children())[:-1]))\n",
    "        self.fc1 = nn.Linear(2048, 4, bias=True)\n",
    "                \n",
    "        for m in self.fc1.parameters():\n",
    "            if isinstance(m, (nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight(), gain=nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inception_layers(x)\n",
    "        x = x.mean((2, 3))\n",
    "        e1  = self.fc1(x)\n",
    "        x  = self.fc1(x)\n",
    "        # e1  = F.relu(self.fc1(x))\n",
    "        # e1 = F.relu(self.fc2(x))\n",
    "        # x = F.dropout(e1, training=self.training)\n",
    "        # x = self.fc2(e1)\n",
    "        return x, e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c264bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_seed(2022)\n",
    "class Strategy:\n",
    "    def __init__(self, dataset, net):\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "\n",
    "    def query(self, n):\n",
    "        pass\n",
    "\n",
    "    def update(self, pos_idxs, neg_idxs=None):\n",
    "        self.dataset.labeled_idxs[pos_idxs] = True\n",
    "        if neg_idxs:\n",
    "            self.dataset.labeled_idxs[neg_idxs] = False\n",
    "\n",
    "#     def train(self, rd, init=False):#在所有label data上train\n",
    "#         labeled_idxs, labeled_data = self.dataset.get_labeled_data()\n",
    "#         val_data = dataset.get_val_data()\n",
    "#         labeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "#         if init==False:\n",
    "#             self.net.supervised_train(rd, labeled_data, val_data)\n",
    "#         else:\n",
    "#             self.net.init_train(unlabeled_data)\n",
    "#         # self.net.supervised_train_loss(rd, labeled_data)\n",
    "#         # self.net.supervised_train_acc(labeled_data)\n",
    "#         # self.net.train(labeled_data)\n",
    "        \n",
    "    def train(self,rd):#在所有label data上train\n",
    "        labeled_idxs, labeled_data = self.dataset.get_labeled_data()\n",
    "#         val_data = dataset.get_val_data()\n",
    "        # self.net.supervised_train(rd, labeled_data, val_data)\n",
    "#         self.net.supervised_val_loss(rd, labeled_data, val_data)\n",
    "        self.net.supervised_train_acc(labeled_data)\n",
    "#         self.net.train(labeled_data)\n",
    "        \n",
    "    def efficient_train(self,rd,data):#在underfit与新image上train\n",
    "        idx = self.net.get_underfit_idx(data)\n",
    "        _, labeled_data  = dataset.get_efficient_training_data(idx,query_idxs)\n",
    "        # val_data = dataset.get_val_data()\n",
    "        # self.net.supervised_train(rd, labeled_data, val_data)\n",
    "        # self.net.supervised_train_loss(rd, labeled_data)\n",
    "        self.net.supervised_train_acc(labeled_data)\n",
    "#         self.net.train(labeled_data)   \n",
    "\n",
    "    def predict(self, data):#在test data上predict\n",
    "        preds = self.net.predict(data)\n",
    "        return preds\n",
    "\n",
    "    def predict_prob(self, data):#\n",
    "        probs = self.net.predict_prob(data)\n",
    "        return probs\n",
    "\n",
    "    def predict_prob_dropout(self, data, n_drop=10):\n",
    "        probs = self.net.predict_prob_dropout(data, n_drop=n_drop)\n",
    "        return probs\n",
    "\n",
    "    def predict_prob_dropout_split(self, data, n_drop=10):\n",
    "        probs = self.net.predict_prob_dropout_split(data, n_drop=n_drop)\n",
    "        return probs\n",
    "    \n",
    "    def get_embeddings(self, data):\n",
    "        embeddings = self.net.get_embeddings(data)\n",
    "        return embeddings\n",
    "    \n",
    "#random sampling\n",
    "class RandomSampling(Strategy):\n",
    "    def __init__(self, dataset, net):\n",
    "        super(RandomSampling, self).__init__(dataset, net)\n",
    "\n",
    "    def query(self, n):\n",
    "        return np.random.choice(np.where(self.dataset.labeled_idxs==0)[0], n, replace=False)\n",
    "\n",
    "#EntropySampling\n",
    "class EntropySampling(Strategy):\n",
    "    def __init__(self, dataset, net):\n",
    "        super(EntropySampling, self).__init__(dataset, net)\n",
    "\n",
    "    def query(self, n):\n",
    "        unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "        probs = self.predict_prob(unlabeled_data)\n",
    "        log_probs = torch.log(probs)\n",
    "        uncertainties = (probs*log_probs).sum(1)\n",
    "        return unlabeled_idxs[uncertainties.sort()[1][:n]]#取最后n个\n",
    "\n",
    "#EntropySamplingDropout\n",
    "class EntropySamplingDropout(Strategy):\n",
    "    def __init__(self, dataset, net, n_drop=10):\n",
    "        super(EntropySamplingDropout, self).__init__(dataset, net)\n",
    "        self.n_drop = n_drop\n",
    "\n",
    "    def query(self, n):\n",
    "        unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "        probs = self.predict_prob_dropout(unlabeled_data, n_drop=self.n_drop)\n",
    "        log_probs = torch.log(probs)\n",
    "        uncertainties = (probs*log_probs).sum(1)\n",
    "        return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "\n",
    "#BALDDropout\n",
    "class BALDDropout(Strategy):\n",
    "    def __init__(self, dataset, net, n_drop=10):\n",
    "        super(BALDDropout, self).__init__(dataset, net)\n",
    "        self.n_drop = n_drop\n",
    "\n",
    "    def query(self, n):\n",
    "        unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "        probs = self.predict_prob_dropout_split(unlabeled_data, n_drop=self.n_drop)#10次求和\n",
    "        pb = probs.mean(0)\n",
    "        entropy1 = (-pb*torch.log(pb)).sum(1)\n",
    "        entropy2 = (-probs*torch.log(probs)).sum(2).mean(0)\n",
    "        uncertainties = entropy2 - entropy1 \n",
    "        return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "\n",
    "import math               \n",
    "#AdversarialDeepFool\n",
    "    def __init__(self, dataset, net, max_iter=10):\n",
    "        super(AdversarialDeepFool, self).__init__(dataset, net)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def cal_dis(self, x):\n",
    "#         image=[]\n",
    "        nx = torch.unsqueeze(x, 0)\n",
    "        nx = nx.cuda()\n",
    "        nx.requires_grad_()\n",
    "        eta = torch.zeros(nx.shape)\n",
    "        eta = eta.cuda()\n",
    "        out, e1 = self.net.clf(nx+eta)\n",
    "        n_class = out.shape[1]\n",
    "        py = out.max(1)[1].item()#最大可能的类别\n",
    "        ny = out.max(1)[1].item()\n",
    "        i_iter = 0\n",
    "\n",
    "        while py == ny and i_iter < self.max_iter:\n",
    "            out[0, py].backward(retain_graph=True)\n",
    "            grad_np = nx.grad.data.clone()\n",
    "            value_l = np.inf\n",
    "            ri = 0\n",
    "\n",
    "            for i in range(n_class):\n",
    "                if i == py:\n",
    "                    continue\n",
    "\n",
    "                nx.grad.data.zero_()\n",
    "                out[0, i].backward(retain_graph=True)\n",
    "                grad_i = nx.grad.data.clone()\n",
    "\n",
    "                wi = grad_i - grad_np\n",
    "                fi = out[0, i] - out[0, py]\n",
    "                value_i = torch.abs(fi) / torch.linalg.norm(wi.flatten())\n",
    "\n",
    "                if value_i < value_l:\n",
    "                    ri = value_i/torch.linalg.norm(wi.flatten()) * wi\n",
    "\n",
    "            eta += ri.clone()\n",
    "            nx.grad.data.zero_()\n",
    "            out, e1 = self.net.clf(nx+eta)\n",
    "            py = out.max(1)[1].item()\n",
    "            i_iter += 1\n",
    "#             image.append((nx+eta).cpu().detach())\n",
    "        image = (nx+eta).cpu().detach()\n",
    "        return (eta*eta).sum(), image, ny #image[-2:]\n",
    "        \n",
    "    def query(self, rd, n):\n",
    "        real_sample = dict()\n",
    "        generative_sample = dict()\n",
    "        pseudo_label = dict()\n",
    "        real_end_sample = []\n",
    "        generative_top_sample = []\n",
    "        generative_end_sample = []\n",
    "        fake_label=[]\n",
    "        uncertainty_index=[]\n",
    "        pseudo_index=[]\n",
    "        unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "        self.net.clf.eval()\n",
    "        dis = np.zeros(unlabeled_idxs.shape)\n",
    "\n",
    "        for i in tqdm(range(len(unlabeled_idxs)), ncols=100):\n",
    "            x, y, idx = unlabeled_data[i]\n",
    "            dis[i], image, label = self.cal_dis(x)\n",
    "            index = unlabeled_idxs[i]\n",
    "            generative_sample[index] = image\n",
    "            real_sample[index] = x.cpu().detach()\n",
    "            pseudo_label[index] = label\n",
    "        #adaptive number\n",
    "        uncertainty_number=int(20-math.exp(0.1*rd))\n",
    "        if uncertainty_number<5:\n",
    "            uncertainty_number=5\n",
    "\n",
    "        pseudo_number=int(math.exp(0.1*rd))+5\n",
    "            \n",
    "        #adaptive percentage\n",
    "#         uncertainty_thresh=np.percentile(dis, 5)#距离短\n",
    "#         pseudo_thresh=np.percentile(dis, 95)\n",
    "# #         distance=np.mean(dis, axis=0)\n",
    "#         print(uncertainty_thresh)\n",
    "#         print(pseudo_thresh)\n",
    "\n",
    "        for i in unlabeled_idxs[dis.argsort()[-pseudo_number:]]:\n",
    "#         for i in unlabeled_idxs[dis>=pseudo_thresh]:\n",
    "            pseudo_index.append(i)\n",
    "            real_end_sample.append(real_sample[i]) #real image\n",
    "#             generative_end_sample.append(generative_sample[i]) #generated image\n",
    "            fake_label.append(pseudo_label[i])\n",
    "\n",
    "        for i in unlabeled_idxs[dis.argsort()[:uncertainty_number]]:\n",
    "#         for i in unlabeled_idxs[dis<=uncertainty_thresh]:\n",
    "            uncertainty_index.append(i)\n",
    "            generative_top_sample.append(generative_sample[i])#list\n",
    "#             torch.save(generative_sample[i], f'./fake_{i}.pth') \n",
    "        return uncertainty_index,generative_top_sample,real_end_sample,fake_label,pseudo_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf9ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'Messidor':\n",
    "              {'n_epoch': 100, \n",
    "               'train_args':{'batch_size': 8, 'num_workers': 4},#对supervised learning与active learning是不同的\n",
    "               'val_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               'test_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               # 'optimizer_args':{'lr': 0.0005, 'weight_decay':0.0005}},#resnet\n",
    "               # 'optimizer_args':{'lr': 0.00005, 'weight_decay':0.0005}},#densenet\n",
    "               'optimizer_args':{'lr': 0.0002, 'weight_decay':0.0005}},#inception paper\n",
    "               # 'optimizer_args':{'lr': 0.0005}},#inception\n",
    "    \n",
    "        'Breast':\n",
    "              {'n_epoch': 100, \n",
    "               'train_args':{'batch_size': 8, 'num_workers': 4},#对supervised learning与active learning是不同的\n",
    "               'val_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               'test_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               # 'optimizer_args':{'lr': 0.0005, 'weight_decay':0.0005}},#resnet\n",
    "               # 'optimizer_args':{'lr': 0.00005, 'weight_decay':0.0005}},#densenet\n",
    "               'optimizer_args':{'lr': 0.0002, 'weight_decay':0.0005}},#inception paper\n",
    "               # 'optimizer_args':{'lr': 0.0005}},#inception\n",
    "    \n",
    "          'Breast_multi':\n",
    "              {'n_epoch': 100, \n",
    "               'train_args':{'batch_size': 8, 'num_workers': 4},#对supervised learning与active learning是不同的\n",
    "               'val_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               'test_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               # 'optimizer_args':{'lr': 0.0005, 'weight_decay':0.0005}},#resnet\n",
    "               # 'optimizer_args':{'lr': 0.00005, 'weight_decay':0.0005}},#densenet\n",
    "               'optimizer_args':{'lr': 0.0002, 'weight_decay':0.0005}},#inception paper\n",
    "               # 'optimizer_args':{'lr': 0.0005}},#inception\n",
    "          }\n",
    "\n",
    "\n",
    "#创建dataset\n",
    "def get_handler(name):\n",
    "    if name == 'Messidor':\n",
    "        return Messidor_Handler\n",
    "    elif name == 'Breast':\n",
    "        return Breast_Handler\n",
    "    elif name == 'Breast_multi':\n",
    "        return Breast_multi_Handler\n",
    "    \n",
    "    \n",
    "#读取数据+handler\n",
    "def get_dataset(name):\n",
    "    if name == 'Messidor':\n",
    "        return get_Messidor(get_handler(name))\n",
    "    if name == 'Breast':\n",
    "        return get_Breast(get_handler(name))\n",
    "    elif name == 'Breast_multi':\n",
    "        return get_Breast_multi(get_handler(name))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "#根据不同数据定义net\n",
    "def get_net(name, device, init=False):\n",
    "    if name == 'CIFAR10':\n",
    "        return Net(CIFAR10_Net, params[name], device)\n",
    "    elif name == 'Messidor':\n",
    "        # return Net(Res_Net, params[name], device)\n",
    "        # if init==False:\n",
    "        return Net(Inception_V3, params[name], device)\n",
    "        # return Net(Dense_Net, params[name], device)\n",
    "#         return Net(Res_Net, params[name], device)\n",
    "        # else:\n",
    "        #     return Net(Autoencoder, params[name], device)\n",
    "    elif name == 'Breast':\n",
    "        # return Net(Res_Net, params[name], device)\n",
    "        # if init==False:\n",
    "        return Net(Inception_V3, params[name], device)\n",
    "    \n",
    "    elif name == 'Breast_multi':\n",
    "        return Net(Inception_V3, params[name], device)\n",
    "#         return Net(Res_Net, params[name], device)\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def get_params(name):\n",
    "    return params[name]\n",
    "\n",
    "def get_strategy(name):\n",
    "    if name == \"RandomSampling\":\n",
    "        return RandomSampling\n",
    "    elif name == \"EntropySampling\":#求entropy作为uncertainty\n",
    "        return EntropySampling\n",
    "    elif name == \"EntropySamplingDropout\":\n",
    "        return EntropySamplingDropout\n",
    "    elif name == \"BALDDropout\":#通过贝叶斯模型 mcdropout得到参数空间，使得参数空间的不确定性越小越好\n",
    "        return BALDDropout\n",
    "    elif name == \"AdversarialDeepFool\":\n",
    "        return AdversarialDeepFool\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6139b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#supervised learning baseline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "# fix random seed\n",
    "setup_seed(2022)\n",
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#对不同的实验组（数据集）要进行得到某一数据集 相应的model和active learning方式\n",
    "X_train, Y_train, X_test, Y_test, handler = get_dataset(args.dataset_name)\n",
    "dataset = Data(X_train, Y_train, X_test, Y_test, handler) \n",
    "# x_train,x_val,y_train,y_val = split_train_val(x_train,y_train)# load dataset\n",
    "# dataset = Data(X_train, Y_train, X_val, Y_val, X_test, Y_test, handler)  \n",
    "net = get_net(args.dataset_name, device)  \n",
    "# start supervised learning baseline\n",
    "dataset.supervised_training_labels()#全部training数据\n",
    "labeled_idxs, labeled_data = dataset.get_labeled_data()\n",
    "# val_data = dataset.get_val_data()\n",
    "net.supervised_train_acc(labeled_data)\n",
    "preds = net.predict(dataset.get_test_data())\n",
    "print(f\"testing accuracy: {dataset.cal_test_acc(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.91it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.78it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.32it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (320, 512, 512, 3)\n",
      "x_test (80, 512, 512, 3)\n",
      "number of labeled pool: 30\n",
      "number of unlabeled pool: 290\n",
      "number of testing pool: 80\n",
      "\n",
      "Round 0\n",
      "labeled data (30,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████▏                                           | 88/300 [03:21<08:06,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 testing accuracy: 0.5375\n",
      "Round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 290/290 [1:34:46<00:00, 19.61s/it]\n",
      "<ipython-input-6-c6461fc8be37>:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X_train = torch.tensor(self.X_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty data 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c6461fc8be37>:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X_train = torch.tensor(self.X_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo data 6\n",
      "labeled data (72,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▋                                  | 131/300 [13:43<17:42,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 testing accuracy: 0.675\n",
      "labeled data (72,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 266/266 [1:31:09<00:00, 20.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty data 18\n",
      "pseudo data 6\n",
      "labeled data (114,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████                                | 143/300 [21:38<23:45,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 testing accuracy: 0.7375\n",
      "labeled data (114,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/242 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 242/242 [1:38:21<00:00, 24.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty data 18\n",
      "pseudo data 6\n",
      "labeled data (156,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████▎                              | 149/300 [29:01<29:24, 11.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3 testing accuracy: 0.775\n",
      "labeled data (156,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 218/218 [1:24:04<00:00, 23.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty data 18\n",
      "pseudo data 6\n",
      "labeled data (198,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████▋                              | 151/300 [35:21<34:52, 14.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4 testing accuracy: 0.825\n",
      "labeled data (198,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 194/194 [1:03:09<00:00, 19.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty data 18\n",
      "pseudo data 6\n",
      "labeled data (240,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████▊                                   | 127/300 [34:51<47:29, 16.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 5 testing accuracy: 0.825\n",
      "labeled data (240,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/170 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████▎                                                      | 20/170 [10:07<44:14, 17.70s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# fix random seed\n",
    "setup_seed(2022)\n",
    "# torch.backends.cudnn.enabled = False\n",
    "accuracy = []\n",
    "size = []\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#对不同的实验组（数据集）要进行得到某一数据集 相应的model和active learning方式\n",
    "   \n",
    "# load dataset\n",
    "X_train, Y_train, X_test, Y_test, handler = get_dataset(args.dataset_name)\n",
    "dataset = Data(X_train, Y_train, X_test, Y_test, handler) \n",
    "\n",
    "net = get_net(args.dataset_name, device, init=False)       # load network\n",
    "strategy = get_strategy(args.strategy_name)(dataset, net)  # load strategy\n",
    "\n",
    "# start active learning experiment\n",
    "dataset.initialize_labels_random(args.n_init_labeled)#随机initialize 10000张\n",
    "#一开始每一类取5000张？一开始模型预测不靠谱 所以要用一些非监督学习方法去获取sample\n",
    "# dataset.initialize_labels(dataset,args.n_init_labeled)\n",
    "# labeled_idxs, _ = dataset.get_labeled_data()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print(labeled_idxs)\n",
    "print(f\"number of labeled pool: {args.n_init_labeled}\")\n",
    "print(f\"number of unlabeled pool: {dataset.n_pool-args.n_init_labeled}\")\n",
    "print(f\"number of testing pool: {dataset.n_test}\")\n",
    "print()\n",
    "\n",
    "# start = time.time()\n",
    "# round 0 accuracy\n",
    "print(\"Round 0\")\n",
    "rd = 0\n",
    "strategy.train(rd)\n",
    "preds = strategy.predict(dataset.get_test_data())\n",
    "print(f\"Round 0 testing accuracy: {dataset.cal_test_acc(preds)}\")#得到test data performance\n",
    "accuracy.append(dataset.cal_test_acc(preds))\n",
    "size.append(args.n_init_labeled)\n",
    "testing_accuracy = 0\n",
    "\n",
    "for rd in range(1, args.n_round+1):\n",
    "    print(f\"Round {rd}\")\n",
    "    # query\n",
    "    if args.strategy_name==\"AdversarialDeepFool\":\n",
    "        query_idxs,generative_top_sample,real_end_sample,fake_label,pseudo_idxs = strategy.query(rd, args.n_query)\n",
    "#         query_idxs,generative_top_sample,real_end_sample,fake_label,pseudo_idxs = strategy.query(rd, args.n_query)#generative_top_sample 20\n",
    "#         query_idxs,generative_top_sample,true_label,real_end_sample,generative_end_sample,fake_label = strategy.query(args.n_query)\n",
    "       \n",
    "        label = dataset.get_label(query_idxs)\n",
    "        adversarial_sample=[]\n",
    "        adversarial_label=[]\n",
    "        pseudo_adversarial_sample=[]\n",
    "        pseudo_label=[]\n",
    "\n",
    "        for i in range(len(query_idxs)):\n",
    "            label = dataset.get_label(query_idxs[i])\n",
    "            dataset.add_labeled_data(generative_top_sample[i],label)\n",
    "        print(\"uncertainty data\",len(query_idxs))\n",
    "        \n",
    "        for i in range(len(pseudo_idxs)):\n",
    "            dataset.update_pseudo_label(pseudo_idxs[i],fake_label[i])\n",
    "            strategy.update(pseudo_idxs)\n",
    "        print(\"pseudo data\",len(pseudo_idxs))\n",
    "\n",
    "    else:\n",
    "        query_idxs = strategy.query(args.n_query)#query_idxs为active learning请求标签的数据\n",
    "        \n",
    "    # update labels\n",
    "    strategy.update(query_idxs)#在原unlabel数据集中减去这部分index\n",
    "    strategy.train(rd)\n",
    "    \n",
    "    # efficient training\n",
    "    # strategy.efficient_train(rd,dataset.get_train_data())\n",
    "    \n",
    "    # calculate accuracy\n",
    "    preds = strategy.predict(dataset.get_test_data())\n",
    "    \n",
    "    testing_accuracy = dataset.cal_test_acc(preds)\n",
    "    print(f\"Round {rd} testing accuracy: {dataset.cal_test_acc(preds)}\")\n",
    "    \n",
    "    accuracy.append(testing_accuracy)\n",
    "    labeled_idxs,_ = dataset.get_labeled_data()\n",
    "    size.append(len(labeled_idxs))\n",
    "    \n",
    "    unlabeled_idxs,_ = dataset.get_unlabeled_data()\n",
    "\n",
    "        \n",
    "dataframe = pd.DataFrame({'model':'Inception_V3','Method':args.strategy_name,'Training dataset size':size,'Accuracy':accuracy})\n",
    "dataframe.to_csv(\"breast_random.csv\",index=False,sep=',')\n",
    "    \n",
    "    # end = time.time()\n",
    "    # print(end-start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3fd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
